# supervisor_agent.py am√©lior√©
import os
import json

class SupervisorAgent:
    def __init__(self):
        self.name = "SupervisorAgent"
        self.rejections = []
        self.rejection_path = "data/output/rejected_predictions.md"
        self.json_output_path = "data/output/final_validated_results.json"
        os.makedirs(os.path.dirname(self.rejection_path), exist_ok=True)

    def is_duplicate(self, citation, seen_texts):
        norm = citation.strip().lower()
        if norm in seen_texts:
            return True
        seen_texts.add(norm)
        return False

    def is_valid(self, citation):
        """
        R√®gles simples pour √©liminer les fausses d√©tections.
        - Ignore les citations tr√®s courtes ou bruit√©es.
        """
        return len(citation.strip()) >= 20 and any(c.isalpha() for c in citation)

    def review_results(self, extracted_mentions, classifier, contextualiser, tracer):
        """
        Orchestration : collecte les r√©sultats de chaque agent, applique des r√®gles de validation
        et enregistre les rejets.

        Args:
            extracted_mentions (list[dict]): Liste des mentions extraites avec index, texte, section, etc.
            classifier, contextualiser, tracer: Agents sp√©cialis√©s.

        Returns:
            list[dict]: R√©sultats consolid√©s et filtr√©s.
        """
        results = []
        seen_texts = set()

        for mention in extracted_mentions:
            index = mention.get("index")
            text = mention.get("text", "").strip()
            section = mention.get("section", None)

            if not self.is_valid(text):
                self._log_rejection(index, text, "Phrase trop courte ou bruit√©e")
                continue
            if self.is_duplicate(text, seen_texts):
                self._log_rejection(index, text, "Doublon d√©tect√©")
                continue

            try:
                data_type = classifier.classify(text, section_name=section)
                if data_type == "inconnu":
                    self._log_rejection(index, text, "Classification incertaine")
                    continue

                context = contextualiser.summarize_context(text)
                source = tracer.trace_source(text)

                result = {
                    "phrase_index": index,
                    "citation_text": text,
                    "type_de_donnee": data_type,
                    "contexte": context,
                    "source": source
                }
                if section:
                    result["section"] = section

                results.append(result)
                print(f"‚úÖ Phrase {index} valid√©e : {data_type} | section={section}")

            except Exception as e:
                self._log_rejection(index, text, f"Erreur d'analyse : {str(e)}")
                continue

        self._save_rejections()
        self._save_json(results)
        return results

    def _log_rejection(self, index, text, reason):
        print(f"‚õîÔ∏è Rejet phrase {index} : {reason}")
        self.rejections.append({
            "index": index,
            "text": text,
            "raison": reason
        })

    def _save_rejections(self):
        if not self.rejections:
            return
        with open(self.rejection_path, "w", encoding="utf-8") as f:
            f.write("# üõë Pr√©dictions rejet√©es\n\n")
            for r in self.rejections:
                f.write(f"## Phrase {r['index']}\n")
                f.write(f"- **Texte** : {r['text']}\n")
                f.write(f"- **Raison du rejet** : {r['raison']}\n\n")

    def _save_json(self, results):
        try:
            with open(self.json_output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"üíæ R√©sultats sauvegard√©s dans {self.json_output_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur de sauvegarde JSON : {e}")
