# utils/sentence_splitter.py â€” Segmentation multilingue robuste
import re
import nltk
from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters
from nltk.tokenize import sent_tokenize

# ðŸ“¦ VÃ©rifie que le tokenizer punkt est dispo
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

def split_sentences(text, lang="fr"):
    """
    Segmente proprement un texte scientifique en phrases, en tenant compte de la langue.

    Args:
        text (str): Texte brut.
        lang (str): 'fr' ou 'en'.

    Returns:
        list[str]: Liste de phrases nettoyÃ©es.
    """
    custom_abbreviations = {"et al", "fig", "dr", "prof", "e.g", "i.e", "etc"}

    # ðŸ§¼ Nettoyage lÃ©ger
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"(?<=[a-zA-Z])-(?=[a-zA-Z])", "", text)  # supprime les cÃ©sures de mots

    # ðŸ”§ Configuration des abrÃ©viations
    punkt_params = PunktParameters()
    punkt_params.abbrev_types = custom_abbreviations
    tokenizer = PunktSentenceTokenizer(punkt_params)

    try:
        if lang == "fr":
            sentences = sent_tokenize(text, language="french")
        elif lang == "en":
            sentences = sent_tokenize(text, language="english")
        else:
            sentences = tokenizer.tokenize(text)
    except Exception:
        # Fallback naÃ¯f si NLTK Ã©choue
        sentences = re.split(r"(?<=[.!?])\s+", text)

    return [s.strip() for s in sentences if len(s.strip()) > 10]
